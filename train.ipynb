{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm torchvision librosa wandb matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "total trainable params 38579138\n",
      "start training data folder <torch.utils.data.dataloader.DataLoader object at 0x14957b470>\n",
      "The learning rate is: 5e-05\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.7008813619613647, ce max 0.7008813619613647, cos min 0.2605772912502289, cos max 0.2605772912502289, norm ce 0.0, norm cos0.0\n",
      "Global Step: 2, Epoch: 1, CE Loss: 0.7008813619613647, Cos Loss: 0.2605772912502289, MSE Loss: 0.5495069026947021, LR: 5e-05: : 1it [00:00,  1.10it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.5867152810096741, ce max 0.7008813619613647, cos min 0.2605772912502289, cos max 0.2902693450450897, norm ce 0.0, norm cos0.9999996632096687\n",
      "Global Step: 3, Epoch: 2, CE Loss: 0.5867152810096741, Cos Loss: 0.2902693450450897, MSE Loss: 0.3962823450565338, LR: 5e-05: : 1it [00:00,  2.03it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 0.7008813619613647, cos min 0.2497827559709549, cos max 0.2902693450450897, norm ce 0.0, norm cos0.0\n",
      "Global Step: 4, Epoch: 3, CE Loss: 0.13282497227191925, Cos Loss: 0.2497827559709549, MSE Loss: 0.5688241124153137, LR: 5e-05: : 1it [00:00,  1.88it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 0.7008813619613647, cos min 0.23733708262443542, cos max 0.2902693450450897, norm ce 0.7312647059731792, norm cos0.0\n",
      "Global Step: 5, Epoch: 4, CE Loss: 0.5482245683670044, Cos Loss: 0.23733708262443542, MSE Loss: 1.085798740386963, LR: 5e-05: : 1it [00:00,  2.46it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 1.3831573724746704, cos min 0.23733708262443542, cos max 0.2902693450450897, norm ce 0.9999999920021269, norm cos0.26077967633621124\n",
      "Global Step: 6, Epoch: 5, CE Loss: 1.3831573724746704, Cos Loss: 0.2511407434940338, MSE Loss: 1.7420787811279297, LR: 5e-05: : 1it [00:00,  2.28it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 1.3831573724746704, cos min 0.23733708262443542, cos max 0.2902693450450897, norm ce 0.3229784262597355, norm cos0.0801928443092252\n",
      "Global Step: 7, Epoch: 6, CE Loss: 0.5366553664207458, Cos Loss: 0.24158187210559845, MSE Loss: 0.9866691827774048, LR: 5e-05: : 1it [00:00,  2.59it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 1.3841705322265625, cos min 0.23733708262443542, cos max 0.2902693450450897, norm ce 0.9999999920086025, norm cos0.6301301880531152\n",
      "Global Step: 8, Epoch: 7, CE Loss: 1.3841705322265625, Cos Loss: 0.27069130539894104, MSE Loss: 1.8444993495941162, LR: 5e-05: : 1it [00:00,  2.36it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 1.3841705322265625, cos min 0.23611414432525635, cos max 0.2902693450450897, norm ce 0.6199729876891117, norm cos0.0\n",
      "Global Step: 9, Epoch: 8, CE Loss: 0.9086254239082336, Cos Loss: 0.23611414432525635, MSE Loss: 1.4552956819534302, LR: 5e-05: : 1it [00:00,  2.35it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.23611414432525635, cos max 0.2902693450450897, norm ce 0.9999999952610268, norm cos0.8858291061565602\n",
      "Global Step: 10, Epoch: 9, CE Loss: 2.2429866790771484, Cos Loss: 0.2840864062309265, MSE Loss: 2.7702622413635254, LR: 5e-05: : 1it [00:00,  2.59it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.23611414432525635, cos max 0.2902693450450897, norm ce 0.220720714298993, norm cos0.10538330946215638\n",
      "Global Step: 11, Epoch: 10, CE Loss: 0.5985813736915588, Cos Loss: 0.24182119965553284, MSE Loss: 1.2073090076446533, LR: 5e-05: : 1it [00:00,  1.94it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.23611414432525635, cos max 0.2902693450450897, norm ce 0.2108086848365836, norm cos0.31524136144326886\n",
      "Global Step: 12, Epoch: 11, CE Loss: 0.577665388584137, Cos Loss: 0.25318610668182373, MSE Loss: 1.0095295906066895, LR: 5e-05: : 1it [00:00,  2.13it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.23156499862670898, cos max 0.2902693450450897, norm ce 0.3549117278851338, norm cos0.0\n",
      "Global Step: 13, Epoch: 12, CE Loss: 0.8817461133003235, Cos Loss: 0.23156499862670898, MSE Loss: 1.271648645401001, LR: 5e-05: : 1it [00:00,  2.42it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.23156499862670898, cos max 0.2902693450450897, norm ce 0.36537168260376646, norm cos0.6100960940961355\n",
      "Global Step: 14, Epoch: 13, CE Loss: 0.9038183093070984, Cos Loss: 0.2673802971839905, MSE Loss: 1.2984626293182373, LR: 5e-05: : 1it [00:00,  2.34it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.23156499862670898, cos max 0.2902693450450897, norm ce 0.18722744836607544, norm cos0.31055142184944207\n",
      "Global Step: 15, Epoch: 14, CE Loss: 0.5279051661491394, Cos Loss: 0.24979571998119354, MSE Loss: 0.42490822076797485, LR: 5e-05: : 1it [00:00,  2.01it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.22780625522136688, cos max 0.2902693450450897, norm ce 0.20319597549517046, norm cos0.0\n",
      "Global Step: 16, Epoch: 15, CE Loss: 0.5616013407707214, Cos Loss: 0.22780625522136688, MSE Loss: 0.42595425248146057, LR: 5e-05: : 1it [00:00,  2.32it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.22780625522136688, cos max 0.2902693450450897, norm ce 0.18395159087224944, norm cos0.27041378819507067\n",
      "Global Step: 17, Epoch: 16, CE Loss: 0.5209925770759583, Cos Loss: 0.24469713866710663, MSE Loss: 0.3938182294368744, LR: 5e-05: : 1it [00:00,  2.17it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.22780625522136688, cos max 0.2902693450450897, norm ce 0.1607724713183097, norm cos0.43748482616503565\n",
      "Global Step: 18, Epoch: 17, CE Loss: 0.47208088636398315, Cos Loss: 0.25513291358947754, MSE Loss: 0.5454829335212708, LR: 5e-05: : 1it [00:00,  2.18it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.22780625522136688, cos max 0.2902693450450897, norm ce 0.39040733042817377, norm cos0.14083975924675265\n",
      "Global Step: 19, Epoch: 18, CE Loss: 0.9566475749015808, Cos Loss: 0.2366035431623459, MSE Loss: 1.2256193161010742, LR: 5e-05: : 1it [00:00,  2.64it/s]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "ce min 0.13282497227191925, ce max 2.2429866790771484, cos min 0.22780625522136688, cos max 0.2902693450450897, norm ce 0.3113747756375923, norm cos0.3950498755999511\n",
      "^C\n",
      "0it [00:00, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/PIL/ImageFile.py\", line 547, in _save\n",
      "    fh = fp.fileno()\n",
      "         ^^^^^^^^^\n",
      "AttributeError: '_idat' object has no attribute 'fileno'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py\", line 673, in <module>\n",
      "    train(device, model, train_data_loader, test_data_loader, optimizer,\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py\", line 433, in train\n",
      "    for step, (x, mel, y) in prog_bar:\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py\", line 281, in __getitem__\n",
      "    save_sample_images(np.concatenate(face_window, axis=2), idx, mel)\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py\", line 340, in save_sample_images\n",
      "    plt.savefig(\"img_{0}_mel_spectrogram.png\".format(idx))\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/matplotlib/pyplot.py\", line 1228, in savefig\n",
      "    res = fig.savefig(*args, **kwargs)  # type: ignore[func-returns-value]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/matplotlib/figure.py\", line 3395, in savefig\n",
      "    self.canvas.print_figure(fname, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/matplotlib/backend_bases.py\", line 2204, in print_figure\n",
      "    result = print_method(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/matplotlib/backend_bases.py\", line 2054, in <lambda>\n",
      "    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n",
      "                                                                 ^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py\", line 496, in print_png\n",
      "    self._print_pil(filename_or_obj, \"png\", pil_kwargs, metadata)\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py\", line 445, in _print_pil\n",
      "    mpl.image.imsave(\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/matplotlib/image.py\", line 1676, in imsave\n",
      "    image.save(fname, **pil_kwargs)\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/PIL/Image.py\", line 2568, in save\n",
      "    save_handler(self, fp, filename)\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/PIL/PngImagePlugin.py\", line 1431, in _save\n",
      "    ImageFile._save(im, _idat(fp, chunk), [(\"zip\", (0, 0) + im.size, 0, rawmode)])\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/PIL/ImageFile.py\", line 551, in _save\n",
      "    _encode_tile(im, fp, tile, bufsize, None, exc)\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/PIL/ImageFile.py\", line 570, in _encode_tile\n",
      "    errcode, data = encoder.encode(bufsize)[1:]\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train syncnet from scrach\n",
    "!python transformer_syncnet_train.py --data_root lrs2_preprocessed_28/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "total trainable params 88766540\n",
      "Load checkpoint from: checkpoints/checkpoint_step000010000.pth\n",
      "/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py:493: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n",
      "The learning rate is: 0.0001\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "Saved checkpoint: checkpoints/checkpoint_step000000001.pth\n",
      "Step: 1, Avg Img Loss: 0.1135554313659668, Sync Loss: 1.199532389640808, Lower Half Loss: 0.13248758018016815, LR: 0.0001: : 1it [00:08,  8.55s/it]\n",
      "0it [00:00, ?it/s]The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "Step: 2, Avg Img Loss: 0.09944925457239151, Sync Loss: 0.7315983772277832, Lower Half Loss: 0.12123202532529831, LR: 0.0001: : 1it [00:07,  7.45s/it]\n",
      "0it [00:00, ?it/s]An error has occured lrs2_preprocessed_28/6120180220883154109/00009 lrs2_preprocessed_28/6120180220883154109/00009/14.jpg lrs2_preprocessed_28/6120180220883154109/00009/30.jpg\n",
      "unsupported operand type(s) for /: 'NoneType' and 'float'\n",
      "The output shape torch.Size([5, 2])\n",
      "The y shape torch.Size([5])\n",
      "Step: 3, Avg Img Loss: 0.09118920564651489, Sync Loss: 0.9754487872123718, Lower Half Loss: 0.10643915086984634, LR: 0.0001: : 1it [00:07,  7.01s/it]\n",
      "0it [00:00, ?it/s]An error has occured lrs2_preprocessed_28/6120180220883154109/00004 lrs2_preprocessed_28/6120180220883154109/00004/25.jpg lrs2_preprocessed_28/6120180220883154109/00004/34.jpg\n",
      "unsupported operand type(s) for /: 'NoneType' and 'float'\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Train original wav2lip from scrach\n",
    "!python transformer_wav2lip_train.py --data_root lrs2_preprocessed_28/ \\\n",
    "--checkpoint_dir checkpoints \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000018000.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
